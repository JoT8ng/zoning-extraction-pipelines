{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61740a76",
   "metadata": {},
   "source": [
    "# Evaluating the Accuracy of the Results from the Claude LLM API Pipeline\n",
    "\n",
    "This pipeline explores extracting the zoning information by first extracting and parsing the text as markdown from the by-law PDFs, then sending a query to the LLM API with the extracted text. The LLM responds with the zoning information and the response is processed and exported into CSV format and joined with a zoning GeoJSON dataset.\n",
    "\n",
    "### What are Zoning By-laws and why do they matter?\n",
    "Zoning By-laws contain important information about land use, building height, density, and other development regulations. They are important documents that inform urban planning and development decisions in cities.\n",
    "\n",
    "They are often stored as long, unstructured PDF legal documents and it's difficult to find information within them. Zoning information is also spatial and tied to geospatial datasets. It would be great if the zoning information in the by-laws could be extracted in an efficient and automated way and joined with geospatial datasets.\n",
    "\n",
    "### Evaluation Metric\n",
    "\n",
    "**After developing this pipeline, its accuracy needs to be evaluated so it can be benchmarked against other models and pipelines. It's important to assess the usefulness, strengths, and weaknesses of different models and pipelines for the desired task.**\n",
    "\n",
    "Although the Exactly Match (EM) and F1 score are metrics most often used to evaluate the accuracy of Question Answering NER models, it makes sense to apply them in this scenario because the LLM is being prompted in such a way as to act like a Question Answering NER model.\n",
    "\n",
    "* **Exact Match (EM):** This metric measures the percentage of questions where the model's answer exactly matches one of the ground truth answers.\n",
    "* **F1 Score:** This metric calculates the overlap between the predicted answer and the ground truth answers. It considers both precision (the number of correct answers provided by the model) and recall (the number of correct answers that should have been provided). The F1 score is the harmonic mean of precision and recall, providing a balance between the two. A higher F1 score indicates a better performing model. The F1 score is good of imbalanced datasets where accuracy can be misleading. [More information](https://www.geeksforgeeks.org/machine-learning/f1-score-in-machine-learning/)\n",
    "\n",
    "A CSV file called \"llm_api_evaluation_dataset.csv\" containing the ground truth and the LLM responses will be used to evaluate the pipeline. For reference, a CSV file (\"example_pipeline_output.csv\") showing the raw output from the pipeline is placed in this repository folder.\n",
    "\n",
    "### Imports and Set Up\n",
    "\n",
    "First, import all the necessary Python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f06975",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dce0e6c",
   "metadata": {},
   "source": [
    "### Evaluation and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745486e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the the evaluation dataset\n",
    "dataset = pd.read_csv(\"llm_api_evaluation_dataset.csv\")\n",
    "\n",
    "# Load SQuAD metrics\n",
    "squad_metric = evaluate.load(\"squad\")\n",
    "\n",
    "# Set up array to store evaluation dataset\n",
    "results = []\n",
    "\n",
    "# WORK IN PROGRESS BELOW\n",
    "# Evaluation helper function to prepare inputs for Hugging Face SQuAD metrics\n",
    "def evaluate_model(res, model):\n",
    "\n",
    "    # res or results: results dictionary containing the outputs of the predictions and ground truth\n",
    "\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    for r in res:\n",
    "        predictions.append({\n",
    "            \"id\": str(r[\"doc_id\"]),\n",
    "            \"prediction_text\": r[model]\n",
    "        })\n",
    "        references.append({\n",
    "            \"id\": str(r[\"doc_id\"]),\n",
    "            \"answers\": {\n",
    "                \"text\": [r[\"ground_truth\"]],\n",
    "                \"answer_start\": [0]  # dummy value\n",
    "            }\n",
    "        })\n",
    "\n",
    "    # Compute metrics\n",
    "    return squad_metric.compute(predictions=predictions, references=references)\n",
    "\n",
    "# Evaluation\n",
    "metrics = evaluate_model(results, \"answer\")\n",
    "\n",
    "print(\"Metrics:\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8685ba7c",
   "metadata": {},
   "source": [
    "### Concluding Thoughts"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
