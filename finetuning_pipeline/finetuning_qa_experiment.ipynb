{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42a24208",
   "metadata": {},
   "source": [
    "# Fine-Tuning RoBERTa on Zoning By-laws\n",
    "\n",
    "### Introduction\n",
    "\n",
    "Zoning By-laws contain important information about land use, building height, density, and other development regulations. They are important documents that inform urban planning and development decisions in cities.\n",
    "\n",
    "They are often stored as long, unstructured PDF legal documents and it's difficult to find information within them. Zoning information is also spatial and tied to geospatial datasets. It would be great if the zoning information in the by-laws could be extracted in an efficient and automated way and joined with geospatial datasets.\n",
    "\n",
    "**This experiement aims to fine-tune a pre-trained RoBERTa QA model to increase its accuracy in extracting information from Zoning By-laws.**\n",
    "\n",
    "### Why RoBERTa?\n",
    "\n",
    "[RoBERTa](https://huggingface.co/docs/transformers/en/model_doc/roberta) is an optimized version of BERT and improves it with new pretraining objectives.  The pretraining objectives include dynamic masking, sentence packing, larger batches and a byte-level BPE tokenizer. Since it is a newer improved model it is generally considered to outperform BERT on NLP tasks. In this experiment a fine-tuned version on SQuAD 2 used for question answering called [roberta-base-squad2 or roberta-base for Extractive QA](https://huggingface.co/deepset/roberta-base-squad2) is used.\n",
    "\n",
    "[roberta-base-squad2 or roberta-base for Extractive QA](https://huggingface.co/deepset/roberta-base-squad2) is chosen because in the [Zeroshot QA Experiments](https://github.com/JoT8ng/zoning-extraction-pipelines/blob/main/zeroshot_qa/zeroshot_qa_experiment2.ipynb) it outperformed [DistilBERT](https://huggingface.co/docs/transformers/en/model_doc/distilbert) in terms of accuracy.\n",
    "\n",
    "For more info on NLP, LLMs, and transformer models:\n",
    "[Hugging Face LLM Course](https://huggingface.co/learn/llm-course/en/chapter1/2)\n",
    "\n",
    "### Fine-Tuning Strategy\n",
    "\n",
    "The fine-tuning strategy chosen for this experiment is based on a paper called [\"Fine-tuning Strategies for Domain Specific Question Answering under Low Annotation Budget Constraints\"](https://arxiv.org/html/2401.09168v1).\n",
    "\n",
    "The paper acknowledges the challenge and cost of adapting foundation models to specific tasks due to the huge amount of annotated samples required to fine-tune those models. In reality, training datasets for domain specific tasks are small due to budget constraints and creating a dataset with hundreds of labelled examples is tedious. \n",
    "\n",
    "The paper highlights that tradtionally this issue is circumvented using a double fine-tuning step:\n",
    "\n",
    "*\"It consists of fine-tuning the pre-trained foundation model on a large-scale training dataset that is as close as possible (domain and objective) to the target task and is then further fine-tuned on the given domain/task for which training data is scarce. The result is a Pre-trained Language Model (PLM) like BERT [1], trained on masked language modeling or text generation task, that is then fine-tuned on a more specific large-scale task (LM’), and ultimately refined on the domain/task at hand (LM’’)...*\n",
    "\n",
    "*In the double fine-tuning step stated above, practitioners usually leverage the Stanford Question Answering Dataset (SQuAD) [10] which is a high-quality QA dataset that covers diverse knowledge for the PLM to train on. Nonetheless, in many real-life scenarios, specific-domain QA has a range of field applications that is narrower than SQuAD and may not appear in the SQuAD training data. This calls for building a domain-specific dataset to further fine-tune a QA model for the domain at hand to produce a QA model LM’’. This last fine-tuning step is domain-dependent, and the practitioner’s goal is also to ultimately keep the number of annotated training samples low - they are under a low annotation budget constraint. It’s worth mentioning that, for extractive QA, annotating 200 examples is already a time-consuming work: the collection of question and answer data requires the annotator to read and understand the text in order to ensure the reasonableness of the marked answers.\"*(Smith & Doe, 2024)\n",
    "\n",
    "The paper concludes that the **best strategy to fine-tune a QA model on low-budget settings is taking a pre-trained model and fine-tuning it with a dataset composed of the target domain dataset and the SQuAD dataset.** This is the strategy that will be used in this experiment.\n",
    "\n",
    "### About the training dataset\n",
    "\n",
    "A dataset of 80 labeled examples was created to use in this experiment. The dataset was created manually from a range of different zoning by-laws from different municipalities across Canada in an Excel document and exported into CSV format. The CSV format is deemed appropriate for this experiment because training dataset is small and simple. [More information on LLM dataset formats](https://huggingface.co/blog/tegridydev/llm-dataset-formats-101-hugging-face)\n",
    "\n",
    "The municipalities whose Zoning By-laws are used for this training dataset:\n",
    "\n",
    "* Toronto\n",
    "* Calgary\n",
    "* Edmonton\n",
    "* Vancouver\n",
    "* Waterloo\n",
    "* Saint John\n",
    "* Surrey\n",
    "\n",
    "To really test the efficacy of the models in extracting the zoning information, a range of different questions and contexts from different zoning by-laws throughout Canada are used. Some of the contexts are a mix of messy and clean snippets from the zoning by-law. One context contains a longer and messy snippet of raw text directly extracted from the pdf.\n",
    "\n",
    "As mentioned above, the fine-tuning strategy involves using a training dataset composed of the target domain dataset and the SQuAD dataset. 50 examples consist of labeled examples from various Zoning By-laws and 30 examples are from the SQuAD dataset.\n",
    "\n",
    "The Hugging Face Datasets library is not used in this experiment because it is not necessary. This is a small experiment and advanced features from the Datasets library (shuffling, splitting, streaming, or pushing to the Hugging Face Hub) are not required.\n",
    "\n",
    "### Imports and Set Up\n",
    "\n",
    "First, import all the necessary Python libraries. The Hugging Face Transformers Library is used.\n",
    "\n",
    "Since this is a small and simple training dataset, an auto tokenizer is used as it is not deemed necessary to manually customize the tokenization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1223d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, RobertaForQuestionAnswering, TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "from datasets import load_dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91aff5af",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "[More information on QA data processing](https://huggingface.co/learn/llm-course/en/chapter7/7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e522cedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV training dataset\n",
    "dataset = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files={\n",
    "        \"train\": \"TrainingDataset.csv\",\n",
    "        \"validation\": \"TestingDataset.csv\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\", use_fast=True)\n",
    "\n",
    "max_length = 512 # maximum number of tokens the model can take as input. For RoBERTa it's 512\n",
    "stride = 128 # When truncating long contexts, stride lets you create a sliding window over the text so overlapping chunks are created\n",
    "\n",
    "# Tokenize function\n",
    "def tokenize_function(examples):\n",
    "\n",
    "    # Tokenize questions and contexts\n",
    "    inputs = tokenizer(\n",
    "        examples[\"question\"],\n",
    "        examples[\"context\"],\n",
    "        truncation = \"only_second\", # Truncation = True will truncate if it exceeds max length. \"only_second\" will truncate only at the second sequence, which is the context\n",
    "        max_length = max_length,\n",
    "        stride = stride,\n",
    "        return_overflowing_tokens = True, # returns extra chunks from stride\n",
    "        return_offsets_mapping = True, # returns a mapping from token positions to character positions in the original text. Can find the start/end of character positions of the answer in context\n",
    "        padding = \"max_length\"\n",
    "    )\n",
    "\n",
    "    # Convert \"ground_truth\" to start/end positions\n",
    "    # In extractive QA, the model is not trained to generate the answer text directly but it learns to predict start token index and end token index inside the given context\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        answer_text = examples[\"ground_truth\"][sample_idx]\n",
    "        context = examples[\"context\"][sample_idx]\n",
    "\n",
    "        # Find character start/end of the answer in the context\n",
    "        start_char = context.find(answer_text)\n",
    "        if start_char == -1:\n",
    "            # If answer not found, set to CLS token (special classification token that represents no answer)\n",
    "            start_positions.append(tokenizer.cls_token_id)\n",
    "            end_positions.append(tokenizer.cls_token_id)\n",
    "            continue\n",
    "        end_char = start_char + len(answer_text)\n",
    "\n",
    "        # Sequence IDs: 0 = question, 1 = context, None = special tokens\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        # Find start/end of the context in token space\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while idx < len(sequence_ids) and sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If answer is outside the context chunk, set CLS\n",
    "        if offsets[context_start][0] > start_char or offsets[context_end][1] < end_char:\n",
    "            start_positions.append(tokenizer.cls_token_id)\n",
    "            end_positions.append(tokenizer.cls_token_id)\n",
    "        else:\n",
    "            # Find start token index\n",
    "            token_start_index = context_start\n",
    "            while token_start_index <= context_end and offsets[token_start_index][0] <= start_char:\n",
    "                token_start_index += 1\n",
    "            start_positions.append(token_start_index - 1)\n",
    "\n",
    "            # Find end token index\n",
    "            token_end_index = context_end\n",
    "            while token_end_index >= context_start and offsets[token_end_index][1] >= end_char:\n",
    "                token_end_index -= 1\n",
    "            end_positions.append(token_end_index + 1)\n",
    "\n",
    "    # Store positions in inputs\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "\n",
    "    # Dataset is ready for fine-tuning\n",
    "    # Contains: input_ids, attention_mask, start_positions_, end_positions\n",
    "    return inputs\n",
    "\n",
    "# Apply preprocessing\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched = True, remove_columns = dataset[\"train\"].column_names)\n",
    "\n",
    "# Check data preprocessing with first 3 samples\n",
    "for idx in range(3):  # first 3 samples\n",
    "    sample = tokenized_datasets[\"train\"][idx]\n",
    "    print(f\"Sample {idx}\")\n",
    "    print(\"Question:\", dataset[\"train\"][idx][\"question\"])\n",
    "    print(\"Context:\", dataset[\"train\"][idx][\"context\"])\n",
    "    print(\"Ground Truth:\", dataset[\"train\"][idx][\"ground_truth\"])\n",
    "    print(\"Start Position:\", sample[\"start_positions\"])\n",
    "    print(\"End Position:\", sample[\"end_positions\"])\n",
    "    print(\"Decoded Answer:\", tokenizer.decode(sample[\"input_ids\"][sample[\"start_positions\"]:sample[\"end_positions\"]+1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e635a1",
   "metadata": {},
   "source": [
    "### Fine-Tune the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addb0f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = RobertaForQuestionAnswering.from_pretrained(\"deepset/roberta-base-squad2\")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./roberta-qa\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "# Define metricsBUG HERE FIX LATER\n",
    "metric = load_metric(\"squad\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    return metric.compute(predictions=p.predictions, references=p.label_ids)\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5eb5f6",
   "metadata": {},
   "source": [
    "### Evaluate the Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98f3865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training metrics\n",
    "logs = pd.DataFrame(trainer.state.log_history)\n",
    "logs.plot(x=\"epoch\", y=[\"loss\", \"eval_loss\"])\n",
    "\n",
    "# Save model\n",
    "trainer.save_model(\"./roberta-qa-finetuned\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
