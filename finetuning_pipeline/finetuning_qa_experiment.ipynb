{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42a24208",
   "metadata": {},
   "source": [
    "# Fine-Tuning RoBERTa on Zoning By-laws\n",
    "\n",
    "### Introduction\n",
    "\n",
    "Zoning By-laws contain important information about land use, building height, density, and other development regulations. They are important documents that inform urban planning and development decisions in cities.\n",
    "\n",
    "They are often stored as long, unstructured PDF legal documents and it's difficult to find information within them. Zoning information is also spatial and tied to geospatial datasets. It would be great if the zoning information in the by-laws could be extracted in an efficient and automated way and joined with geospatial datasets.\n",
    "\n",
    "**This experiement aims to fine-tune a pre-trained RoBERTa QA model to increase its accuracy in extracting information from Zoning By-laws.**\n",
    "\n",
    "### Why RoBERTa?\n",
    "\n",
    "[RoBERTa](https://huggingface.co/docs/transformers/en/model_doc/roberta) is an optimized version of BERT and improves it with new pretraining objectives.  The pretraining objectives include dynamic masking, sentence packing, larger batches and a byte-level BPE tokenizer. Since it is a newer improved model it is generally considered to outperform BERT on NLP tasks. In this experiment a fine-tuned version on SQuAD 2 used for question answering called [roberta-base-squad2 or roberta-base for Extractive QA](https://huggingface.co/deepset/roberta-base-squad2) is used.\n",
    "\n",
    "[roberta-base-squad2 or roberta-base for Extractive QA](https://huggingface.co/deepset/roberta-base-squad2) is chosen because in the [Zeroshot QA Experiments](https://github.com/JoT8ng/zoning-extraction-pipelines/blob/main/zeroshot_qa/zeroshot_qa_experiment2.ipynb) it outperformed [DistilBERT](https://huggingface.co/docs/transformers/en/model_doc/distilbert) in terms of accuracy.\n",
    "\n",
    "For more info on NLP, LLMs, and transformer models:\n",
    "[Hugging Face LLM Course](https://huggingface.co/learn/llm-course/en/chapter1/2)\n",
    "\n",
    "### Fine-Tuning Strategy\n",
    "\n",
    "The fine-tuning strategy chosen for this experiment is based on a paper called [\"Fine-tuning Strategies for Domain Specific Question Answering under Low Annotation Budget Constraints\"](https://arxiv.org/html/2401.09168v1).\n",
    "\n",
    "The paper acknowledges the challenge and cost of adapting foundation models to specific tasks due to the huge amount of annotated samples required to fine-tune those models. In reality, training datasets for domain specific tasks are small due to budget constraints and creating a dataset with hundreds of labelled examples is tedious. \n",
    "\n",
    "The paper highlights that tradtionally this issue is circumvented using a double fine-tuning step:\n",
    "\n",
    "*\"It consists of fine-tuning the pre-trained foundation model on a large-scale training dataset that is as close as possible (domain and objective) to the target task and is then further fine-tuned on the given domain/task for which training data is scarce. The result is a Pre-trained Language Model (PLM) like BERT [1], trained on masked language modeling or text generation task, that is then fine-tuned on a more specific large-scale task (LM’), and ultimately refined on the domain/task at hand (LM’’)...*\n",
    "\n",
    "*In the double fine-tuning step stated above, practitioners usually leverage the Stanford Question Answering Dataset (SQuAD) [10] which is a high-quality QA dataset that covers diverse knowledge for the PLM to train on. Nonetheless, in many real-life scenarios, specific-domain QA has a range of field applications that is narrower than SQuAD and may not appear in the SQuAD training data. This calls for building a domain-specific dataset to further fine-tune a QA model for the domain at hand to produce a QA model LM’’. This last fine-tuning step is domain-dependent, and the practitioner’s goal is also to ultimately keep the number of annotated training samples low - they are under a low annotation budget constraint. It’s worth mentioning that, for extractive QA, annotating 200 examples is already a time-consuming work: the collection of question and answer data requires the annotator to read and understand the text in order to ensure the reasonableness of the marked answers.\"*(Smith & Doe, 2024)\n",
    "\n",
    "The paper concludes that the **best strategy to fine-tune a QA model on low-budget settings is taking a pre-trained model and fine-tuning it with a dataset composed of the target domain dataset and the SQuAD dataset.** This is the strategy that will be used in this experiment.\n",
    "\n",
    "### About the training dataset\n",
    "\n",
    "A dataset of 200 labeled examples for training and 50 labeled examples for validation was created to use in this experiment. The dataset was created manually from a range of different zoning by-laws from different municipalities across Canada in an Excel document and exported into CSV format. The CSV format is deemed appropriate for this experiment because training dataset is small and simple. [More information on LLM dataset formats](https://huggingface.co/blog/tegridydev/llm-dataset-formats-101-hugging-face)\n",
    "\n",
    "The municipalities whose Zoning By-laws are used for the training dataset:\n",
    "\n",
    "* Toronto\n",
    "* Calgary\n",
    "* Edmonton\n",
    "* Vancouver\n",
    "* Waterloo\n",
    "* Saint John\n",
    "* Surrey\n",
    "* Burnaby\n",
    "* City of Langley\n",
    "* Halifax\n",
    "* Niagara Falls\n",
    "* Maple Ridge\n",
    "\n",
    "To really test the efficacy of the models in extracting the zoning information, a range of different questions and contexts from different zoning by-laws throughout Canada are used. Some of the contexts are a mix of messy and clean snippets from the zoning by-law. Some contexts contain a longer snippet of raw text directly extracted from the by-law to optimize how the model understands local context within a larger relevant document. Training with longer truncated inputs provides better contextual representations.\n",
    "\n",
    "As mentioned above, the fine-tuning strategy involves using a training dataset composed of the target domain dataset and the SQuAD dataset. 150 examples consist of labeled examples from various Zoning By-laws and 50 examples are from the [SQuAD 2.0 dataset](https://rajpurkar.github.io/SQuAD-explorer/).\n",
    "\n",
    "The Hugging Face Datasets library is not used in this experiment because it is not necessary. This is a small experiment and advanced features from the Datasets library (shuffling, splitting, streaming, or pushing to the Hugging Face Hub) are not required.\n",
    "\n",
    "### Imports and Set Up\n",
    "\n",
    "First, import all the necessary Python libraries. The Hugging Face Transformers Library is used.\n",
    "\n",
    "Since this is a small and simple training dataset, an auto tokenizer is used as it is not deemed necessary to manually customize the tokenization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1223d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, RobertaForQuestionAnswering, TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "from datasets import load_dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91aff5af",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "[More information on QA data processing](https://huggingface.co/learn/llm-course/en/chapter7/7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e522cedf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1d14b0c4dfc40598626922256bef17c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30dcaf04de304cfcb038da2273a0b93b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c714b50bf9094f6b8d2d0b7df24c14d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8886ef3a7b0d4d80a9e38241cfdb3b23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0\n",
      "Question: What is the maximum building depth?\n",
      "Context: The maximum building depth of a Contextual Semi-detached Dwelling is the greater of: A 60.0 per cent of the parcel depth; or B the contextual building depth average.\n",
      "Ground Truth: 60.0 per cent of the parcel depth; or B the contextual building depth average.\n",
      "Start Position: 30\n",
      "End Position: 47\n",
      "Decoded Answer:  60.0 per cent of the parcel depth; or B the contextual building depth average.\n",
      "Validation Sample 0\n",
      "Question: What is the minimum setback?\n",
      "Context: Minimum Setback for portions of Towers greater than 23.0 m in Height from an Abutting Street: 6.0 m\n",
      "Ground Truth: 6.0 m\n",
      "Start Position: 31\n",
      "End Position: 34\n",
      "Decoded Answer:  6.0 m\n",
      "Sample 1\n",
      "Question: What is the maximum building depth for a Contextual Single Detached Dwelling located on a parcel width greather than 10 metres?\n",
      "Context: The maximum building depth of a Contextual Semi-detached Dwelling is the greater of: A 60.0 per cent of the parcel depth; or B the contextual building depth average. Where a Contextual Single Detached Dwelling is located on a parcel with a parcel width greater than 10 metres the maximum building depth is the contextual building depth average.\n",
      "Ground Truth: contextual building depth average.\n",
      "Start Position: 62\n",
      "End Position: 66\n",
      "Decoded Answer:  contextual building depth average.\n",
      "Validation Sample 1\n",
      "Question: What is the minimum site area?\n",
      "Context: Minimum Site area 1.0 ha\n",
      "Ground Truth: 1.0 ha\n",
      "Start Position: 13\n",
      "End Position: 16\n",
      "Decoded Answer:  1.0 ha\n",
      "Sample 2\n",
      "Question: What is the maximum building depth for a building containing one unit?\n",
      "Context: Unless otherwise referenced in subsections (2) and (3) the maximum building depth is 65.0 per cent of the parcel depth for a building containing a unit. On a laned parcel, there is no maximum building depth for a main residential building wholly contained to the rear of 40.0 per cent parcel depth where: there is more than one main residential building on the parcel; \n",
      "Ground Truth: 65.0 per cent of the parcel depth\n",
      "Start Position: 33\n",
      "End Position: 41\n",
      "Decoded Answer:  65.0 per cent of the parcel depth\n",
      "Validation Sample 2\n",
      "Question: What is the maximum height?\n",
      "Context: Maximum Height: 12.0 m\n",
      "Ground Truth: 12.0 m\n",
      "Start Position: 12\n",
      "End Position: 15\n",
      "Decoded Answer:  12.0 m\n"
     ]
    }
   ],
   "source": [
    "# Load CSV training dataset\n",
    "dataset = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files={\n",
    "        \"train\": \"TrainingDataset.csv\",\n",
    "        \"validation\": \"ValidationDataset.csv\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\", use_fast = True)\n",
    "\n",
    "max_length = 512 # maximum number of tokens the model can take as input. For RoBERTa it's 512\n",
    "stride = 128 # When truncating long contexts, stride lets you create a sliding window over the text so overlapping chunks are created\n",
    "\n",
    "# Tokenize function\n",
    "def tokenize_function(examples):\n",
    "\n",
    "    # Tokenize questions and contexts\n",
    "    inputs = tokenizer(\n",
    "        examples[\"question\"],\n",
    "        examples[\"context\"],\n",
    "        truncation = \"only_second\", # Truncation = True will truncate if it exceeds max length. \"only_second\" will truncate only at the second sequence, which is the context\n",
    "        max_length = max_length,\n",
    "        stride = stride,\n",
    "        return_overflowing_tokens = True, # returns extra chunks from stride\n",
    "        return_offsets_mapping = True, # returns a mapping from token positions to character positions in the original text. Can find the start/end of character positions of the answer in context\n",
    "        padding = \"max_length\"\n",
    "    )\n",
    "\n",
    "    # Convert \"ground_truth\" to start/end positions\n",
    "    # In extractive QA, the model is not trained to generate the answer text directly but it learns to predict start token index and end token index inside the given context\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        answer_text = str(examples[\"ground_truth\"][sample_idx])\n",
    "        context = examples[\"context\"][sample_idx]\n",
    "\n",
    "        cls_index = inputs[\"input_ids\"][i].index(tokenizer.cls_token_id)\n",
    "\n",
    "        # Find character start/end of the answer in the context\n",
    "        start_char = context.find(answer_text)\n",
    "        if start_char == -1:\n",
    "            # If answer not found, set to CLS token (special classification token that represents no answer)\n",
    "            start_positions.append(cls_index)\n",
    "            end_positions.append(cls_index)\n",
    "            continue\n",
    "        end_char = start_char + len(answer_text)\n",
    "\n",
    "        # Sequence IDs: 0 = question, 1 = context, None = special tokens\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        # Find the start and end of the context in token space\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while idx < len(sequence_ids) and sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context chunk, set CLS\n",
    "        if offsets[context_start][0] > start_char or offsets[context_end][1] < end_char:\n",
    "            start_positions.append(cls_index)\n",
    "            end_positions.append(cls_index)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            # Find start token index\n",
    "            token_start_index = context_start\n",
    "            while token_start_index <= context_end and offsets[token_start_index][0] <= start_char:\n",
    "                token_start_index += 1\n",
    "            start_positions.append(token_start_index - 1)\n",
    "\n",
    "            # Find end token index\n",
    "            token_end_index = context_end\n",
    "            while token_end_index >= context_start and offsets[token_end_index][1] >= end_char:\n",
    "                token_end_index -= 1\n",
    "            end_positions.append(token_end_index + 1)\n",
    "\n",
    "    # Store positions in inputs\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "\n",
    "    # Dataset is ready for fine-tuning\n",
    "    # Contains: input_ids, attention_mask, start_positions_, end_positions\n",
    "    return inputs\n",
    "\n",
    "# Apply preprocessing\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched = True, remove_columns = dataset[\"train\"].column_names)\n",
    "\n",
    "# Check data preprocessing for training and validation datasets with first 3 samples\n",
    "for idx in range(3):  # first 3 samples\n",
    "    sample = tokenized_datasets[\"train\"][idx]\n",
    "    print(f\"Sample {idx}\")\n",
    "    print(\"Question:\", dataset[\"train\"][idx][\"question\"])\n",
    "    print(\"Context:\", dataset[\"train\"][idx][\"context\"])\n",
    "    print(\"Ground Truth:\", dataset[\"train\"][idx][\"ground_truth\"])\n",
    "    print(\"Start Position:\", sample[\"start_positions\"])\n",
    "    print(\"End Position:\", sample[\"end_positions\"])\n",
    "    print(\"Decoded Answer:\", tokenizer.decode(sample[\"input_ids\"][sample[\"start_positions\"]:sample[\"end_positions\"]+1]))\n",
    "    vsample = tokenized_datasets[\"validation\"][idx]\n",
    "    print(f\"Validation Sample {idx}\")\n",
    "    print(\"Question:\", dataset[\"validation\"][idx][\"question\"])\n",
    "    print(\"Context:\", dataset[\"validation\"][idx][\"context\"])\n",
    "    print(\"Ground Truth:\", dataset[\"validation\"][idx][\"ground_truth\"])\n",
    "    print(\"Start Position:\", vsample[\"start_positions\"])\n",
    "    print(\"End Position:\", vsample[\"end_positions\"])\n",
    "    print(\"Decoded Answer:\", tokenizer.decode(vsample[\"input_ids\"][vsample[\"start_positions\"]:vsample[\"end_positions\"]+1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e635a1",
   "metadata": {},
   "source": [
    "### Fine-Tune the Model\n",
    "\n",
    "Since the training dataset is small, I have decided to train the model with 5 epochs. A failsafe \"early stopping callback with patience\" is added in the code so that if the model starts showing signs of overfitting during training it will stop after 2 epochs. Early stopping is a technique that monitors a metric during training and stops when the metric stops improving after a certain number of evaluation steps to prevent overfitting and save compute time.\n",
    "\n",
    "The learning rate is set to a slower \"2e-5\" to prevent overfitting of the model because of the small dataset. [More information on learning rates](https://www.ibm.com/think/topics/learning-rate#:~:text=Learning%20rate%20is%20important%20because,small%20that%20learning%20is%20inefficient.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addb0f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = RobertaForQuestionAnswering.from_pretrained(\"deepset/roberta-base-squad2\")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = \"./roberta-qa\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate = 2e-5,\n",
    "    per_device_train_batch_size = 4,\n",
    "    per_device_eval_batch_size = 4,\n",
    "    num_train_epochs = 5,\n",
    "    weight_decay = 0.01,\n",
    "    load_best_model_at_end = True,\n",
    "    metric_for_best_model = \"eval_loss\",\n",
    "    greater_is_better = False,\n",
    "    logging_dir = \"./logs\",\n",
    "    logging_steps = 10,\n",
    "    save_strategy = \"epoch\"\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = tokenized_datasets[\"train\"],\n",
    "    eval_dataset = tokenized_datasets[\"validation\"],\n",
    "    processing_class = tokenizer,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5eb5f6",
   "metadata": {},
   "source": [
    "### Understanding Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98f3865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training metrics\n",
    "logs = pd.DataFrame(trainer.state.log_history)\n",
    "logs = logs.dropna(subset=[\"epoch\"])\n",
    "logs.plot(x=\"epoch\", y=[\"loss\", \"eval_loss\"])\n",
    "\n",
    "# Save model\n",
    "trainer.save_model(\"./roberta-zoning-qa-finetuned1\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
